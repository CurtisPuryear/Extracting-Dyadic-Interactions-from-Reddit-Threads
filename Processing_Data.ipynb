{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UtfqKLlak9K"
      },
      "outputs": [],
      "source": [
        "#grabbing data first (only in Google Collab)\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acmNJDk7UKOK",
        "outputId": "ae78351f-b2ca-4d36-8ce8-52e22cdf9287"
      },
      "outputs": [],
      "source": [
        "# Link Google Drive with Collab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6rrs-7Fav-Z"
      },
      "outputs": [],
      "source": [
        "#importing important packages\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9_xgR8UEWh"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"drive/My Drive/understanding_files/merged_data_allsubs_wtext.csv\", encoding_errors= 'replace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnXQd3GvUEWh"
      },
      "outputs": [],
      "source": [
        "# Global variables zone:\n",
        "#     Tweak these to change little details about how the program runs\n",
        "\n",
        "FILE_SAVE_LOCATION: str = \"drive/My Drive/understanding_files/\"      # Each post will saved into this folder as an individual .csv named after its post id.\n",
        "CMT_CHAIN_LEN: int = 4                                        # Filter for comment chains of the length specified here.\n",
        "IMPORTANT_COLUMNS: list = [\"cmt_id\",\"submission_title\",\"text\",\"submission_link_id\",\"created_utc\",\"author\",\"author_id\",\"cmt_link_id\",\"cmt_parent_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA2ymRbhUEWi"
      },
      "outputs": [],
      "source": [
        "# Function definition zone\n",
        "\n",
        "def remove_t(element):\n",
        "  \"\"\"Remove the 't3_' 't2_' and 't1_' from all cells that have them as a prefix to the data we want to use. - andre.chiquit.ooo\"\"\"\n",
        "  if type(element) == str:\n",
        "    if element[2] == \"_\":\n",
        "      return (element[3:])\n",
        "    else: \n",
        "      return (element)\n",
        "  else:\n",
        "    return (element)\n",
        "\n",
        "\n",
        "def pull_post(data: pd.DataFrame, post_id: str) -> pd.DataFrame:\n",
        "  \"\"\"Creates a DataFrame out of a specific post_id.\"\"\"\n",
        "  return data[data['submission_link_id']==post_id]\n",
        "\n",
        "\n",
        "def find_top_level_comments(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Finds all the top level comments.\"\"\"\n",
        "  return  data[data[\"cmt_parent_id\"] == data[\"submission_link_id\"]]\n",
        "\n",
        "\n",
        "def find_children(data: pd.DataFrame, parent_id: str) -> pd.DataFrame:\n",
        "  \"\"\"Finds all the child comments from a certain comment.\"\"\"\n",
        "  return data[data[\"cmt_parent_id\"]==parent_id]\n",
        "\n",
        "\n",
        "def assemble_children(data: pd.DataFrame, top_comment_id: str, parent_df: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Creates a data frame for all responses to top level comment.\"\"\"\n",
        "  children = find_children(data, top_comment_id)\n",
        "  parent_df = parent_df.append(children, ignore_index=True)\n",
        "  for child in children[\"cmt_id\"]:\n",
        "    parent_df=assemble_children(data, child, parent_df)\n",
        "  return parent_df\n",
        "\n",
        "\n",
        "def multi_data_frame(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Function to turn data frame into a 3d data frame by parent comment. This one is kinda cool, but it makes the data wayyyy harder to work with.\"\"\"\n",
        "  top_cmts=find_top_level_comments(data)\n",
        "  top_comment_names=top_cmts[\"cmt_id\"]\n",
        "  top_comment_names.name=\"top_comment_id\"\n",
        "  all_dfs=[]\n",
        "  for top_cmt in top_cmts[\"cmt_id\"]:\n",
        "    starting_df=data[data[\"cmt_id\"]==top_cmt]\n",
        "    comment_replies=assemble_children(data,top_cmt,starting_df)\n",
        "    all_dfs.append(comment_replies)\n",
        "  final_dfs = pd.concat(all_dfs, axis=0, keys=top_comment_names, ignore_index=False)\n",
        "  return final_dfs\n",
        "\n",
        "\n",
        "def is_not_top(comment: pd.DataFrame) -> bool:\n",
        "  \"\"\"Checks whether a comment is a top level comment.\"\"\"\n",
        "  if (comment[\"cmt_parent_id\"].equals(comment[\"submission_link_id\"])):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "\n",
        "def grab_parent_comment(data: pd.DataFrame, current_comment: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"grabs parent comment and returns it.\"\"\"\n",
        "  # print(f\"Variable: data            - the entire dataframe for the post in question           : {data}\")\n",
        "  # print(f\"Variable: current_comment - a dataframe with a single row that's the active comment : {current_comment}\")\n",
        "  if (is_not_top(current_comment)):\n",
        "    return data[data[\"cmt_id\"]==current_comment[\"cmt_parent_id\"].item()]\n",
        "  else:\n",
        "    return\n",
        "\n",
        "\n",
        "def get_parent_cmt_ids(data: pd.DataFrame, current_comment: pd.DataFrame, comment_chain) -> list:\n",
        "  \"\"\"function to add comment id to chain.\"\"\"\n",
        "  parent = grab_parent_comment(data, current_comment)\n",
        "  \n",
        "  if parent is not None:\n",
        "    # print(f\"Variable: data            - the entire dataframe for the post in question (I think...) : {data}\")\n",
        "    # print(f\"Variable: current_comment - a dataframe with a single row that's the active comment    : {current_comment}\")\n",
        "    # print(f\"Variable: comment_chain   - a list with comment IDs in it                              : {comment_chain}\") # Debug??\n",
        "    # print(f\"Variable: parent          - a dataframe resulting from running grab_parent_comment     : {parent}\")\n",
        "    comment_chain.append(parent[\"cmt_id\"].item())\n",
        "    for par in parent[\"cmt_id\"]:\n",
        "      get_parent_cmt_ids(data,data[data[\"cmt_id\"]==par],comment_chain)\n",
        "  return comment_chain\n",
        "\n",
        "\n",
        "def get_parent_author_ids(data: pd.DataFrame, current_comment: pd.DataFrame, author_chain) -> list:\n",
        "  \"\"\"function to add author id to chain.\"\"\"\n",
        "  parent = grab_parent_comment(data, current_comment)\n",
        "  \n",
        "  if parent is not None:\n",
        "    author_chain.append(parent[\"author_id\"].item())\n",
        "    for par in parent[\"cmt_id\"]:\n",
        "      get_parent_author_ids(data,data[data[\"cmt_id\"]==par],author_chain)\n",
        "  return author_chain\n",
        "\n",
        "\n",
        "def add_cmt_aut_chains(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Adds comment and author chain to regular data frame.\"\"\"\n",
        "  final_data=data\n",
        "  final_data[\"cmt_chain\"]=[[] for _ in range(final_data.shape[0])]\n",
        "  final_data[\"author_chain\"]=[[] for _ in range(final_data.shape[0])]\n",
        "  for comment in data[\"cmt_id\"]:\n",
        "      this_comment=data[data[\"cmt_id\"]==comment]\n",
        "      \n",
        "      #initialize comment and author chain variables\n",
        "      cmt_chain=[]\n",
        "      aut_chain=[]\n",
        "      cmt_chain.append(this_comment[\"cmt_id\"].item())\n",
        "      aut_chain.append(this_comment[\"author_id\"].item())\n",
        "\n",
        "      #create chains\n",
        "      comment_chain=get_parent_cmt_ids(data,this_comment,cmt_chain)\n",
        "      author_chain=get_parent_author_ids(data,this_comment,aut_chain)\n",
        "\n",
        "      #add chains to data\n",
        "      final_data.at[final_data[final_data[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"cmt_chain\"]=comment_chain\n",
        "      final_data.at[final_data[final_data[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"author_chain\"]=author_chain\n",
        "  return final_data\n",
        "\n",
        "\n",
        "def remove_orphans(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Remove comments that have no parent.\"\"\"\n",
        "  truthValues=[]\n",
        "  # print(f\"Data currently has {len(data)} comments\")\n",
        "  for par in data[\"cmt_parent_id\"]:\n",
        "    if data[\"cmt_id\"].str.contains(par).any() | data[\"submission_link_id\"].str.contains(par).any():\n",
        "      truthValues.append(True)\n",
        "    else:\n",
        "      truthValues.append(False)\n",
        "  data = data[truthValues]\n",
        "  # print(truthValues)\n",
        "  # print(f\"After removing orphans, data now has {len(data)} comments\")\n",
        "  if False in truthValues:\n",
        "    data = remove_orphans(data)\n",
        "  return data\n",
        "\n",
        "\n",
        "def add_unique_authors(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Add unique authors column.\"\"\"\n",
        "  data=data.reset_index(drop = True)\n",
        "  for i in range(len(data.index)):\n",
        "    data.at[i,\"unique_authors\"]=list(dict.fromkeys(data.at[i,\"author_chain\"]))\n",
        "  return data\n",
        "\n",
        "\n",
        "def add_cmt_aut_chain_strings(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Adds author and comment chains as string.\"\"\"\n",
        "  final_df=data\n",
        "  for comment in data[\"cmt_id\"]:\n",
        "    this_comment=data[data[\"cmt_id\"]==comment]\n",
        "    final_df.at[final_df[final_df[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"cmt_chain_string\"]=\"_\".join(this_comment[\"cmt_chain\"].item())\n",
        "    final_df.at[final_df[final_df[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"author_chain_string\"]=\"_\".join(this_comment[\"author_chain\"].item())\n",
        "  return final_df\n",
        "\n",
        "def troubleshooting_add_cmt_aut_chain_strings(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Adds author and comment chains as string.\"\"\"\n",
        "  final_df=data\n",
        "  counter: int = 0\n",
        "  for comment in data[\"cmt_id\"]:\n",
        "    print(f\"Currently processing [{counter}/{len(data.index)}]\")\n",
        "    this_comment=data[data[\"cmt_id\"]==comment]\n",
        "    final_df.at[final_df[final_df[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"cmt_chain_string\"]=\"_\".join(this_comment[\"cmt_chain\"].item())\n",
        "    final_df.at[final_df[final_df[\"cmt_id\"]==this_comment[\"cmt_id\"].item()].index.item(),\"author_chain_string\"]=\"_\".join(this_comment[\"author_chain\"].item())\n",
        "    counter += 1\n",
        "  return final_df\n",
        "\n",
        "\n",
        "def is_not_found_later(data: pd.DataFrame, comment_chain_str: str):\n",
        "  \"\"\"Determine if string of comments is in a later comment.\"\"\"\n",
        "  is_not_found=True\n",
        "  temp_data=data[data[\"cmt_chain_string\"] != comment_chain_str]\n",
        "  for cmt_chain_str in temp_data[\"cmt_chain_string\"]:\n",
        "    if comment_chain_str in cmt_chain_str:\n",
        "      is_not_found=False\n",
        "  return is_not_found\n",
        "\n",
        "\n",
        "def remove_duped_comment_chains(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Remove comment chains that are found later.\"\"\"\n",
        "  truthValues=[]\n",
        "  for cmt_chain in data[\"cmt_chain_string\"]:\n",
        "    if is_not_found_later(data,cmt_chain):\n",
        "      truthValues.append(True)\n",
        "    else:\n",
        "      truthValues.append(False)\n",
        "  return data[truthValues]\n",
        "\n",
        "\n",
        "def pull_all_conversations(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Pull all conversations that are at least 2 unique authors.\"\"\"\n",
        "  # The output sometimes ends up with conversations that are just two unique authors, but more often it seems to have many more than two. I'm going to leave this code as is\n",
        "  #     and write a new function to filter the number of authors down to two \n",
        "  final_convos=data[data[\"unique_authors\"].map(len)==2]\n",
        "\n",
        "  messy_convos=data[data[\"unique_authors\"].map(len)>2]\n",
        "  for mess in messy_convos[\"cmt_id\"]:\n",
        "    author_chain=messy_convos[messy_convos[\"cmt_id\"]==mess][\"author_chain\"].item()\n",
        "    unique_set=messy_convos[messy_convos[\"cmt_id\"]==mess][\"unique_authors\"].item()[0:2] #this grabs the first two authors which is all we care about\n",
        "    i=0\n",
        "    while(author_chain[i] in unique_set):\n",
        "      i=i+1\n",
        "    new_comment_line=messy_convos[messy_convos[\"cmt_id\"]==mess]\n",
        "    new_comment_line.at[new_comment_line.index.item(),\"cmt_chain\"]=new_comment_line[\"cmt_chain\"].item()[0:i]\n",
        "    new_comment_line.at[new_comment_line.index.item(),\"author_chain\"]=new_comment_line[\"author_chain\"].item()[0:i]\n",
        "    final_convos=pd.concat([final_convos,new_comment_line],ignore_index=True)\n",
        "\n",
        "  final_convos=add_cmt_aut_chain_strings(final_convos)\n",
        "  \n",
        "  final_convos=remove_duped_comment_chains(final_convos)\n",
        "  \n",
        "  return final_convos\n",
        "\n",
        "\n",
        "def add_cmt_chain_len(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Adds a column denoting the length of each comment chain. - andre.chiquit.ooo\"\"\"\n",
        "  data=data.reset_index(drop = True)\n",
        "  for i in range(len(data.index)):\n",
        "    x = len(data.at[i,\"cmt_chain\"])\n",
        "    data.at[i,\"cmt_chain_len\"] = x\n",
        "  return data\n",
        "\n",
        "def unique_cells(data: pd.DataFrame) -> list:\n",
        "  \"\"\"Returns a list of every unique value in a dataframe. If you want to use this on one column, which you probably do, make sure to call it on one column. Enables us to avoid using groupby. - andre.chiquit.ooo\"\"\"\n",
        "  authors: list = []\n",
        "  x: int = 0\n",
        "  for element in data:\n",
        "    if (x % 250000) == 0:\n",
        "      if x < 1000000:\n",
        "        print(f\".{str(x)[:2]} million rows out of ~{str(len(data.index))[:1]}.{str(len(data.index))[1:2]} million total rows processed.\")\n",
        "      else:\n",
        "        print(f\"{str(x)[:1]}.{str(x)[1:3]} million rows out of ~{str(len(data.index))[:1]}.{str(len(data.index))[1:2]} million total rows processed.\")\n",
        "    x += 1\n",
        "    if element not in authors:\n",
        "      authors.append(element)\n",
        "  return (authors)\n",
        "\n",
        "def add_convo_metadata(data: pd.DataFrame, convos:pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Takes a dataframe of comments and adds columns to each comment to include metadata on the conversations to which they belong, taken from a second dataframe. - andre.chiquit.ooo\"\"\"\n",
        "  x: int = 0\n",
        "  z: int = 0\n",
        "  data.reset_index(inplace = True, drop = True)\n",
        "  convos.reset_index(inplace = True, drop = True)\n",
        "  for i in range(len(convos.index)):\n",
        "    x = 0\n",
        "    while x < len(convos.at[i, \"cmt_chain\"]):\n",
        "      for comment in convos.at[i, \"cmt_chain\"]:\n",
        "        z = 0\n",
        "        while z < len(data.index):\n",
        "          if data.at[z, \"cmt_id\"] == comment:\n",
        "            data.at[z, \"convo_id\"] = convos.at[i, \"cmt_chain_string\"]\n",
        "            data.at[z, \"unique_authors\"] = convos.at[i, \"unique_authors\"]\n",
        "            data.at[z, \"cmt_chain_len\"] = convos.at[i, \"cmt_chain_len\"]\n",
        "            z += len(data.index)\n",
        "          z += 1\n",
        "      x += 1\n",
        "  return data\n",
        "\n",
        "def cmt_chain_len_filter(data: pd.DataFrame, convo_len: int) -> pd.DataFrame:\n",
        "  \"\"\"Filters a dataframe for comment chains of a certain length. - andre.chiquit.ooo\"\"\"\n",
        "  mask = []\n",
        "  for i in range(len(data.index)):\n",
        "    if data.at[i, \"cmt_chain_len\"] < convo_len:\n",
        "      mask.append(False)\n",
        "    else:\n",
        "      mask.append(True)\n",
        "  data = data[mask]\n",
        "  return data\n",
        "\n",
        "def dyadic_convo_filter(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Filters a dataframe for only those conversations that have two authors. - andre.chiquit.ooo\"\"\"\n",
        "  mask = []\n",
        "  for i in range(len(data.index)):\n",
        "    if len(data.at[i, \"unique_authors\"]) == 2:\n",
        "      mask.append(True)\n",
        "    else:\n",
        "      mask.append(False)\n",
        "  data = data[mask]\n",
        "  return data\n",
        "\n",
        "def filter_out_comments(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"Filters out any comments that aren't related to the identified conversations and organizes them based on conversation and creation time. - andre.chiquit.ooo\"\"\"\n",
        "  mask = []\n",
        "  data.reset_index(inplace = True, drop = True)\n",
        "  for i in range(len(data.index)):\n",
        "    if type(data.at[i, \"convo_id\"]) == str:\n",
        "      mask.append(True)\n",
        "    else:\n",
        "      mask.append(False)\n",
        "  data = data[mask]\n",
        "  data = data.sort_values(by =['convo_id', 'created_utc'])\n",
        "  return data\n",
        "\n",
        "def processing_for_coding(data: pd.DataFrame) -> None:\n",
        "  \"\"\"Function to use on a dataset to convert it to a dataset with comment chain conversations grouped, ready to be coded. Does not make use of groupby.\"\"\"\n",
        "  print(\"Filtering out for important columns only.\")\n",
        "  data = data[IMPORTANT_COLUMNS]\n",
        "  data = data.dropna()\n",
        "  print(\"Removing 't1_', 't2_', and 't3_' from the cells in which they appear\")\n",
        "  data[['author_id','cmt_link_id','cmt_parent_id']]=data[['author_id','cmt_link_id','cmt_parent_id']].applymap(remove_t)\n",
        "  print(\"Making a list of all the post IDs included in the data\")\n",
        "  post_ids = unique_cells(data[\"submission_link_id\"])\n",
        "  post_index: int = 1\n",
        "  posts = len(post_ids)\n",
        "  print(\"Time to process each post... Hang on!\")\n",
        "  for post in post_ids:\n",
        "    print(f\"Currently processing post_id {post} ({post_index}/{posts}). {posts - post_index} posts left . . .\")\n",
        "    data1 = pull_post(data, post)\n",
        "    if len(data1.index) >= 1:\n",
        "      data1 = remove_orphans(data1)\n",
        "      data1 = add_cmt_aut_chains(data1)\n",
        "      data1 = add_unique_authors(data1)\n",
        "      data1 = add_cmt_aut_chain_strings(data1)\n",
        "      data_convos = pull_all_conversations(data1)\n",
        "      data_convos = add_cmt_chain_len(data_convos)\n",
        "      data_convos = dyadic_convo_filter(data_convos)\n",
        "      data_convos = cmt_chain_len_filter(data_convos, CMT_CHAIN_LEN)\n",
        "      if len(data_convos.index) > 0:\n",
        "        data1 = add_convo_metadata(data1, data_convos)\n",
        "        data1.reset_index(drop = True, inplace = True)\n",
        "        data1 = filter_out_comments(data1)\n",
        "        data1.to_csv(FILE_SAVE_LOCATION + f\"{post}_conversations.csv\")\n",
        "    post_index += 1\n",
        "  return None\n",
        "\n",
        "def processing_for_coding_after_list(data: pd.DataFrame, post_ids: list) -> None:\n",
        "  \"\"\"Uses the list of post id method rather than groupby.\"\"\"\n",
        "  post_index: int = 1\n",
        "  posts = len(post_ids)\n",
        "  print(\"Time to process each post... Hang on!\")\n",
        "  for post in post_ids:\n",
        "    if (post_index % 50) == 0:\n",
        "      print(f\"Currently processing post_id {post} ({post_index}/{posts}). {posts - post_index} posts left . . .\")\n",
        "    data1 = pull_post(data, post)\n",
        "    if len(data1.index) >= 1:\n",
        "      data1 = remove_orphans(data1)\n",
        "      data1 = add_cmt_aut_chains(data1)\n",
        "      data1 = add_unique_authors(data1)\n",
        "      data1 = add_cmt_aut_chain_strings(data1)\n",
        "      data_convos = pull_all_conversations(data1)\n",
        "      data_convos = add_cmt_chain_len(data_convos)\n",
        "      data_convos = dyadic_convo_filter(data_convos)\n",
        "      data_convos = cmt_chain_len_filter(data_convos, CMT_CHAIN_LEN)\n",
        "      if len(data_convos.index) > 0:\n",
        "        data1 = add_convo_metadata(data1, data_convos)\n",
        "        data1.reset_index(drop = True, inplace = True)\n",
        "        data1 = filter_out_comments(data1)\n",
        "        data1.to_csv(FILE_SAVE_LOCATION + f\"{post}_conversations.csv\")\n",
        "    post_index += 1\n",
        "  return None\n",
        "\n",
        "def processing_posts(data: pd.DataFrame) -> None:\n",
        "  \"\"\"Processes each post. Meant for use with groupby. Work in progress\"\"\"\n",
        "  print(\"Made it all the way here!\")\n",
        "  print(data)\n",
        "  data = remove_orphans(data)\n",
        "  print(\"I guess it removed the orphans, man\")\n",
        "  data = add_cmt_aut_chains(data)\n",
        "  data = add_unique_authors(data)\n",
        "  data = add_cmt_aut_chain_strings(data)\n",
        "  data_convos = pull_all_conversations(data)\n",
        "  data_convos = add_cmt_chain_len(data_convos)\n",
        "  data_convos = dyadic_convo_filter(data_convos)\n",
        "  data_convos = cmt_chain_len_filter(data_convos, CMT_CHAIN_LEN)\n",
        "  if len(data_convos.index) > 0:\n",
        "    data = add_convo_metadata(data, data_convos)\n",
        "    data.reset_index(drop = True, inplace = True)\n",
        "    data = filter_out_comments(data)\n",
        "    data.to_csv(FILE_SAVE_LOCATION + f\"{data.at[0, 'submission_link_id']}_conversations.csv\")\n",
        "  return None\n",
        "\n",
        "\n",
        "def processing_for_coding_groupby(data: pd.DataFrame) -> None:\n",
        "  \"\"\"Processes Reddit comments and makes use of the groupby method. Work in progress.\"\"\"\n",
        "  print(\"Filtering out for important columns only.\")\n",
        "  data = data[IMPORTANT_COLUMNS]\n",
        "  data = data.dropna()\n",
        "  print(\"Removing 't1_', 't2_', and 't3_' from the cells in which they appear\")\n",
        "  data[['author_id','cmt_link_id','cmt_parent_id']]=data[['author_id','cmt_link_id','cmt_parent_id']].applymap(remove_t)\n",
        "  groups = data.groupby(['submission_link_id'], sort = False)\n",
        "  groups.transform(processing_posts)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgyURSjnWPjD"
      },
      "source": [
        "The code below is trying to make groupby work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9mKkt2eWSf7"
      },
      "outputs": [],
      "source": [
        "processing_for_coding_groupby(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko-h6k3uw5J2"
      },
      "source": [
        "The code below will work, but it will take a while. It would be more efficient to use groupby, which I was playing with creating a function to do that above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzCr1aJ2SjXP",
        "outputId": "c8e05168-6f49-4a09-f59f-2299c982ef22"
      },
      "outputs": [],
      "source": [
        "print(\"Filtering out for important columns only.\")\n",
        "data = data[IMPORTANT_COLUMNS]\n",
        "data = data.dropna()\n",
        "print(\"Removing 't1_', 't2_', and 't3_' from the cells in which they appear\")\n",
        "data[['author_id','cmt_link_id','cmt_parent_id']]=data[['author_id','cmt_link_id','cmt_parent_id']].applymap(remove_t)\n",
        "print(\"Making a list of all the post IDs included in the data\")\n",
        "post_ids = unique_cells(data[\"submission_link_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6scMBfqRSiLD"
      },
      "outputs": [],
      "source": [
        "processing_for_coding_after_list(data, post_ids)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e20f00ac0587888dbd112b3a8f7b10ca1b00cf2d235cdb21caeecc17b95fcf2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
